{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we are going to make practical use of what we just learned with using high performance computing to explore a region of the genome, the RHD locus, which is responsible for the Rh antigen that can have an adverse effect during some pregnancies. This gene is located at chr1:25598977-25656936 in the hg19 version of the human genome reference sequence. We are going to use Python to help manage our data as it is well suited for this.\n",
    "\n",
    "We are also going to make use of some genomics software packages and data formats that were just introduced, specifically *samtools* and *bam format*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first set imported packages and specific file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import re\n",
    "from pylab import plot,show\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "#update to your own folder\n",
    "baseDir='/scratch/biobootcamp_fluxod/remills/day4/read_counts_by_region'\n",
    "os.chdir(baseDir)\n",
    "\n",
    "#take the baseDir and concatentate on individual file names\n",
    "fnSampleInfo = '%s/%s'%( baseDir, 'sample_info.txt' )\n",
    "fnSampleList = '%s/%s'%( baseDir, 'sample_names.txt' )\n",
    "\n",
    "#full path to samtools package\n",
    "samtools = '/sw/med/centos7/samtools/0.1.19/samtools'\n",
    "\n",
    "#define some empty global dictionaries\n",
    "samplePath={}\n",
    "samplePop={}\n",
    "    \n",
    "#define region of interest\n",
    "chromo = \"1\"\n",
    "startPos = 25598977\n",
    "endPos = 25656936\n",
    "region = \"1:25598977-25656936\" #format needed for calling samtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these packages should be familiar from Tuesday (e.g. matplotlib, numpy, scipi), but a few may be new. \n",
    "- *re* is a package that allows the use of regular expressions\n",
    "- *os* allows for the use of operating specific commands (e.g mkdir)\n",
    "- *subprocess* allows us to construct more complicated system calls, which we will need when creating our samtools command line\n",
    "\n",
    "Let's take a quick look at the sample info file to check it's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileSampleInfo = open(fnSampleInfo,'r')\n",
    "line = fileSampleInfo.readline()\n",
    "fileSampleInfo.close()\n",
    "\n",
    "line = line.rstrip()\n",
    "line = line.split('\\t')\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file appears to contain various metadata for a number of individual. We are particularly interested in two fields, namely *Sample* which gives a unique identifier for an individual, and *File Path* which gives an absolute location to the alignment file for each respective sample. To facilitate the input of this information into a data structure we can make use of, let's define a function to read through the file line by line and store the data for each sample and its file location into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseSampleTable( filename ):\n",
    "    fileSampTable = open(filename,'r')\n",
    "    \n",
    "    # skip past the file header\n",
    "    fileSampTable.readline()\n",
    "\n",
    "    #define dictionaries to store the data we read in\n",
    "    samplePath={}\n",
    "    samplePop={}\n",
    "    \n",
    "    # go line by line\n",
    "    for line in fileSampTable:\n",
    "        #first strip off the *end of line* character with rstrip\n",
    "        #then split the line into a list, using the tab character '\\t' as delimiter\n",
    "        line = line.rstrip().split('\\t')\n",
    "        \n",
    "        #since we know what the indices are for our variables of interest, we can access them directly\n",
    "        samplePath[ line[0] ] = line[5]\n",
    "        samplePop[ line[0] ] = line[2]\n",
    "    \n",
    "    #return our two dictionaries for use elsewhere in the notebook\n",
    "    return samplePath, samplePop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call this function using the file name variable we defined above and assign it to our previously defined global dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplePath, samplePop = parseSampleTable( fnSampleInfo )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out with one of our sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "sample = \"HG00112\"\n",
    "samplePath[sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to count the sequence reads at the RHD locus from each file and compare them.  We can do this using the *samtools* command we talked about earlier along with the unix command *wc -l* which will count the number of lines printed to thes screen. We can call this program from within python using the *subprocess.call()* function\n",
    "\n",
    "As a review, *samtools* has the following usage:\n",
    "*samtools view [options] <in.bam>|<in.sam> [region1 [...]]*\n",
    "\n",
    "So, we need to give it a path to the in.bam file and the region we are interested in. Since *samtools* is a standalone software package, we can construct our command line in Python and run it as though we were using the command line directly using *subprocess.call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#construct command as if we would be running it from the command line. \n",
    "command = samtools + \" view \" + samplePath[sample] + \" \" + region + \" | wc -l > \" + sample + \".cnt\"\n",
    "print(command)\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of *0* indicates that there were no errors returned when running this system call\n",
    "\n",
    "With this command, we:\n",
    "- Pulled out the sequence reads at position 1:25598977-25656936 from the bam file of interest\n",
    "- *piped* the results to the *wc -l* command, which returns the number of lines\n",
    "- redirected the output to a new file using *>*\n",
    "\n",
    "Please take a moment to write some code in the block below to open this file in Jupyter and look at it's output (*hint*, you can reuse some code we made earlier in the notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual genome may be sequenced to different depths of coverage, leading to differences in read counts being a reflection of that depth rather than a true difference in sequence content. Thus, it would make sense to normalize by the total number of reads generated either for the whole genome or, as we will do here, for the chromosome of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only need data for chromosome 1, so 'head -1' returns only the first line of the output that corresponds to chromo 1\n",
    "#Number of aligned reads is in column 3, so we can 'cut' this column out and be left with only a single data point\n",
    "command = samtools + \" idxstats \" + samplePath[sample] + \" | head -1 | cut -f 3 > \" + sample + \".num\"\n",
    "print(command)\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with over 2500 sequences it would be inefficient (and boring!) to do all of them manually in this fashion. We could make use of cluster computing to run all 2500 simultaneously, however this would result in 2500 individual jobs that would need to be submitted, managed, and reviewed.\n",
    "\n",
    "Instead, we can *batch* samples into smaller collections and send each batch as its own job. To do this, we will need to determine both **(a)** the number of batches we wish to submit and **(b)** the number of samples to include in each batch. For this exercise, we will construct batches of n=50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with over 2500 sequences it would be inefficient (and boring!) to do all of them manually in this fashion. We could make use of cluster computing to run all 2500 simultaneously, however this would result in 2500 individual jobs that would need to be submitted, managed, and reviewed.\n",
    "\n",
    "Instead, we can *batch* samples into smaller collections and send each batch as its own job. We can do this by creating a series of batch files containing the commands we wish to run, and then submit them as a *job array* to the cluster.\n",
    "\n",
    "Let's iterate through our samplePath dictionary and construct our commands for each as above, writing them as needed to their respective batch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set target number of batches (jobs)\n",
    "numBatches = 50\n",
    "\n",
    "#length of dictionary = number of samples here, can be calculated with len(dict)\n",
    "numSamplesPerBatch = int(len(samplePath) / numBatches) + 1; #add one to round up if needed\n",
    "\n",
    "#show summary\n",
    "print(len(samplePath))\n",
    "print(numSamplesPerBatch)\n",
    "\n",
    "#keep track of what the last sample number was\n",
    "lastSampleNum = 0;\n",
    "#create a list of sample ids, to match the sample paths\n",
    "samples = list(samplePath.keys())\n",
    "\n",
    "#create a directory to store all our batch, log, and output files\n",
    "if not os.path.exists(baseDir + \"/batches\"):\n",
    "    os.makedirs(baseDir + \"/batches\") \n",
    "\n",
    "#create a directory to store stdout and stderr files\n",
    "if not os.path.exists(baseDir + \"/logs\"):\n",
    "    os.makedirs(baseDir + \"/logs\") \n",
    "    \n",
    "if not os.path.exists(baseDir + \"/counts\"):\n",
    "    os.makedirs(baseDir + \"/counts\")\n",
    "\n",
    "    \n",
    "for batchNum in range (numBatches):\n",
    "    #open a command file for this batch\n",
    "    fileBatch = open(baseDir+\"/batches/batch\"+str(batchNum)+\".sh\", \"w\")\n",
    "    \n",
    "    #set the range for this batch\n",
    "    startIndex = lastSampleNum\n",
    "    endIndex = lastSampleNum + numSamplesPerBatch\n",
    "    \n",
    "    #make sure last index is in range, in case batches aren't divided evenly\n",
    "    if endIndex > len(samplePath): \n",
    "        endIndex = len(samplePath)\n",
    "        \n",
    "    for sampleNum in range (startIndex, endIndex):\n",
    "        sample = samples[sampleNum] #current sample\n",
    "        #write command for region counts\n",
    "        command = samtools + \" view \" + samplePath[sample] + \" \" + region + \" | wc -l > \" + baseDir + \"/counts/\" + sample + \".cnt\"\n",
    "        fileBatch.write(command+\"\\n\")\n",
    "        #write command for chromosome counts\n",
    "        command = samtools + \" idxstats \" + samplePath[sample] + \" | head -1 | cut -f 3 > \" + baseDir + \"/counts/\" + sample + \".num\"\n",
    "        fileBatch.write(command+\"\\n\")\n",
    "    fileBatch.close()\n",
    "    \n",
    "    #update index for next iteration\n",
    "    lastSampleNum = lastSampleNum + numSamplesPerBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use python to create a parent PBS file that we can use for all batch files and subsequently submit them to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new PBS file for submission, with corresponding header\n",
    "filePBS = open(baseDir+\"/day4.pbs\", \"w\")\n",
    "filePBS.write(\"#!/bin/bash\\n\") #shell line\n",
    "filePBS.write(\"#\\n\") #spacer\n",
    "filePBS.write(\"#PBS -N bioboot_day4\\n\") #name of job\n",
    "filePBS.write(\"#PBS -t 0-49\\n\") #set the number of batches to run\n",
    "filePBS.write(\"#PBS -o \" + baseDir + \"/logs/batch.stdout\\n\") #stdout file\n",
    "filePBS.write(\"#PBS -e \" + baseDir + \"/logs/batch.stderr\\n\") #stderr file\n",
    "filePBS.write(\"#PBS -l procs=1,qos=flux,mem=4gb,walltime=24:00:00\\n\") #feature line\n",
    "filePBS.write(\"#PBS -m a\\n\") #message line, only send errors to email\n",
    "filePBS.write(\"#PBS -M remills@umich.edu\\n\") #email line, to send messages from above\n",
    "filePBS.write(\"#PBS -A biobootcamp_fluxod\\n\") #which account to associate job\n",
    "filePBS.write(\"#PBS -q fluxod\\n\") #which queue to send job to \n",
    "filePBS.write(\"#PBS -V\\n\") #pass environmental variables to job\n",
    "filePBS.write(\"#PBS -d .\\n\") #use current working directory\n",
    "filePBS.write(\"\\n\") #spacer\n",
    "\n",
    "filePBS.write(\"bash \" + baseDir + \"/batches/batch${PBS_ARRAYID}.sh\\n\")\n",
    "filePBS.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit our job array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "command = \"qsub day4.pbs\"\n",
    "print(command)\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must wait for the jobs to finish. There are python modules which will check and sleep until all jobs are finalized. For simplicity, we can just manually check with the *qstat* command you were introduced to earlier (use your unique name instead of mine!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!qstat -u remills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view all jobs in the array as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!qstat -t 24299940[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the status of an individual job itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -t 24299940[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take a few minutes to run. Once completed, we should take the time to do some quick checks of the stderr files to make sure that there were not  issues with any particular batch. This can be as simple as a quick *cat* of all the stderr files to see if anything is there. The lack of errors sent to your email address is also a sign that things went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat {baseDir}/logs/*stderr*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use python to read back in all the output files and construct a normalized list of sequence counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = samplePath.keys()\n",
    "rhdList = []\n",
    "for sample in samples:\n",
    "    #rhd counts\n",
    "    cntFile = open(baseDir + \"/counts/\" + sample + \".cnt\", \"r\")\n",
    "    cnt = cntFile.readline()\n",
    "    cnt = float(cnt.rstrip()) #float is important, otherwise will treat as STR or INT\n",
    "    cntFile.close()\n",
    "    \n",
    "    #chromo 1 counts\n",
    "    numFile = open(baseDir + \"/counts/\" + sample + \".num\", \"r\")\n",
    "    num = numFile.readline()\n",
    "    num = float(num.rstrip())\n",
    "    numFile.close()\n",
    "    \n",
    "    normCnt = 1e3 * (cnt / num) #scale for plotting\n",
    "    rhdList.append(normCnt)\n",
    "    print(rhdList[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rhdList, bins=100)\n",
    "plt.title(\"RHD Locus Read Counts\")\n",
    "plt.xlabel(\"Sequence Read Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our samples seem to stratify into three distinct groups based on their depth of sequence coverage at the RHD locus, which represent individuals with 2 (normal) copies of the region, 1 copy, and 0 copies. The last peak is much smaller, and is consistent with observed frequencies of Rh- individuals (~10-15%)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
